{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "There are several main adjustments you may try.\n",
    "\n",
    "Please pay attention to the time it takes for each epoch to conclude.\n",
    "\n",
    "Using the code from the lecture as the basis, fiddle with the hyperparameters of the algorithm.\n",
    "\n",
    "1. The *width* (the hidden layer size) of the algorithm. Try a hidden layer size of 200. How does the validation accuracy of the model change? What about the time it took the algorithm to train? Can you find a hidden layer size that does better?\n",
    "\n",
    "2. The *depth* of the algorithm. Add another hidden layer to the algorithm. This is an extremely important exercise! How does the validation accuracy change? What about the time it took the algorithm to train? Hint: Be careful with the shapes of the weights and the biases.\n",
    "\n",
    "3. The *width and depth* of the algorithm. Add as many additional layers as you need to reach 5 hidden layers. Moreover, adjust the width of the algorithm as you find suitable. How does the validation accuracy change? What about the time it took the algorithm to train?\n",
    "\n",
    "4. Fiddle with the activation functions. Try applying sigmoid transformation to both layers. The sigmoid activation is given by the string 'sigmoid'.\n",
    "\n",
    "5. Fiddle with the activation functions. Try applying a ReLu to the first hidden layer and tanh to the second one. The tanh activation is given by the string 'tanh'.\n",
    "\n",
    "6. Adjust the batch size. Try a batch size of 10000. How does the required time change? What about the accuracy?\n",
    "\n",
    "7. Adjust the batch size. Try a batch size of 1. That's the SGD. How do the time and accuracy change? Is the result coherent with the theory?\n",
    "\n",
    "8. Adjust the learning rate. Try a value of 0.0001. Does it make a difference?\n",
    "\n",
    "9. Adjust the learning rate. Try a value of 0.02. Does it make a difference?\n",
    "\n",
    "10. Combine all the methods above and try to reach a validation accuracy of 98.5+ percent.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST Classification\n",
    "\n",
    "We'll apply all the knowledge from the lectures in this section to write a deep neural network. The problem we've chosen is referred to as the \"Hello World\" of deep learning because for most students it is the first deep learning algorithm they see.\n",
    "\n",
    "The dataset is called MNIST and refers to handwritten digit recognition. You can find more about it on Yann LeCun's website (Director of AI Research, Facebook). He is one of the pioneers of what we've been talking about and of more complex approaches that are widely used today, such as covolutional neural networks (CNNs). \n",
    "\n",
    "The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image). \n",
    "\n",
    "The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes. \n",
    "\n",
    "Our goal would be to build a neural network with 2 hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\py3-TF2.0_ver3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# TensorFLow includes a data provider for MNIST that we'll use.\n",
    "# It comes with the tensorflow-datasets module, therefore, if you haven't please install the package using\n",
    "# pip install tensorflow-datasets \n",
    "# or\n",
    "# conda install tensorflow-datasets\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# these datasets will be stored in C:\\Users\\*USERNAME*\\tensorflow_datasets\\...\n",
    "# the first time you download a dataset, it is stored in the respective folder \n",
    "# every other time, it is automatically loading the copy on your computer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "That's where we load and preprocess our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\s0001033\\tensorflow_datasets\\mnist\\3.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/2 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/3 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Size...:   0%|          | 0/1 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Size...:   0%|          | 0/1 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Size...:   0%|          | 0/1 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:00,  3.59 url/s]\n",
      "Dl Size...:   0%|          | 0/1 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  3.59 url/s]\n",
      "Dl Size...:   0%|          | 0/1 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  3.59 url/s]\n",
      "Dl Size...:   0%|          | 0/1 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  3.59 url/s]]\u001b[A\u001b[A\n",
      "Dl Size...:   0%|          | 0/1 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...:   0%|          | 0/2 [00:00<?, ? file/s]\u001b[A\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  3.59 url/s]9 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:   0%|          | 0/1 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  3.59 url/s]9 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:   0%|          | 0/1 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 2/2 [00:00<00:00,  3.09 file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  3.59 url/s]\n",
      "Dl Size...: 100%|██████████| 1/1 [00:00<00:00,  2.59 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  7.40 url/s]9 file/s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 1/1 [00:00<00:00,  2.59 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  7.40 url/s]9 file/s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 1/1 [00:00<00:00,  2.59 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  7.40 url/s]9 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:  10%|█         | 1/10 [00:00<00:03,  2.59 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...:  67%|██████▋   | 2/3 [00:00<00:00,  3.09 file/s]\u001b[A\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  7.40 url/s]5 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:  10%|█         | 1/10 [00:00<00:03,  2.59 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 3/3 [00:00<00:00,  6.05 file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  7.40 url/s]\n",
      "Dl Size...:  20%|██        | 2/10 [00:00<00:02,  3.30 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 3/3 [00:00<00:00,  6.05 file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  7.40 url/s]\n",
      "Dl Size...:  30%|███       | 3/10 [00:00<00:01,  4.75 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  7.40 url/s]5 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:  40%|████      | 4/10 [00:00<00:01,  4.75 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 3/3 [00:00<00:00,  6.05 file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  7.40 url/s]\n",
      "Dl Size...:  50%|█████     | 5/10 [00:00<00:00,  7.05 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:01<00:00,  7.40 url/s]5 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:  60%|██████    | 6/10 [00:01<00:00,  7.05 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 3/3 [00:01<00:00,  6.05 file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:01<00:00,  7.40 url/s]\n",
      "Dl Size...:  70%|███████   | 7/10 [00:01<00:00,  8.14 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:01<00:00,  7.40 url/s]5 file/s]\u001b[A\u001b[A\n",
      "Dl Size...:  80%|████████  | 8/10 [00:01<00:00,  8.14 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 3/3 [00:01<00:00,  6.05 file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:01<00:00,  7.40 url/s]\n",
      "Dl Size...:  90%|█████████ | 9/10 [00:01<00:00,  8.93 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:01<00:00,  7.40 url/s]5 file/s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 10/10 [00:01<00:00,  8.93 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:01<00:00,  2.31 url/s]5 file/s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 10/10 [00:01<00:00,  8.93 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:01<00:00,  2.31 url/s]5 file/s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 10/10 [00:01<00:00,  8.93 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...:  75%|███████▌  | 3/4 [00:01<00:00,  6.05 file/s]\u001b[A\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:01<00:00,  2.31 url/s]3 file/s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 10/10 [00:01<00:00,  8.93 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 4/4 [00:01<00:00,  2.20 file/s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 10/10 [00:01<00:00,  5.50 MiB/s]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:01<00:00,  2.20 url/s]\n",
      "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]\n",
      "Generating train examples...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Generating train examples...: 1 examples [00:01,  1.32s/ examples]\u001b[A\n",
      "Generating train examples...: 245 examples [00:01, 238.53 examples/s]\u001b[A\n",
      "Generating train examples...: 466 examples [00:01, 478.94 examples/s]\u001b[A\n",
      "Generating train examples...: 693 examples [00:01, 743.82 examples/s]\u001b[A\n",
      "Generating train examples...: 929 examples [00:01, 1024.63 examples/s]\u001b[A\n",
      "Generating train examples...: 1151 examples [00:01, 1263.17 examples/s]\u001b[A\n",
      "Generating train examples...: 1390 examples [00:01, 1513.01 examples/s]\u001b[A\n",
      "Generating train examples...: 1637 examples [00:02, 1739.10 examples/s]\u001b[A\n",
      "Generating train examples...: 1867 examples [00:02, 1875.70 examples/s]\u001b[A\n",
      "Generating train examples...: 2104 examples [00:02, 2004.54 examples/s]\u001b[A\n",
      "Generating train examples...: 2344 examples [00:02, 2113.45 examples/s]\u001b[A\n",
      "Generating train examples...: 2591 examples [00:02, 2207.90 examples/s]\u001b[A\n",
      "Generating train examples...: 2843 examples [00:02, 2290.88 examples/s]\u001b[A\n",
      "Generating train examples...: 3084 examples [00:02, 2312.06 examples/s]\u001b[A\n",
      "Generating train examples...: 3328 examples [00:02, 2349.07 examples/s]\u001b[A\n",
      "Generating train examples...: 3569 examples [00:02, 2346.56 examples/s]\u001b[A\n",
      "Generating train examples...: 3808 examples [00:02, 2331.65 examples/s]\u001b[A\n",
      "Generating train examples...: 4047 examples [00:03, 2345.31 examples/s]\u001b[A\n",
      "Generating train examples...: 4284 examples [00:03, 2327.35 examples/s]\u001b[A\n",
      "Generating train examples...: 4528 examples [00:03, 2360.20 examples/s]\u001b[A\n",
      "Generating train examples...: 4778 examples [00:03, 2401.51 examples/s]\u001b[A\n",
      "Generating train examples...: 5019 examples [00:03, 2389.89 examples/s]\u001b[A\n",
      "Generating train examples...: 5259 examples [00:03, 2358.18 examples/s]\u001b[A\n",
      "Generating train examples...: 5501 examples [00:03, 2376.03 examples/s]\u001b[A\n",
      "Generating train examples...: 5740 examples [00:03, 2373.23 examples/s]\u001b[A\n",
      "Generating train examples...: 5978 examples [00:03, 2375.23 examples/s]\u001b[A\n",
      "Generating train examples...: 6225 examples [00:03, 2402.31 examples/s]\u001b[A\n",
      "Generating train examples...: 6472 examples [00:04, 2422.14 examples/s]\u001b[A\n",
      "Generating train examples...: 6715 examples [00:04, 2399.25 examples/s]\u001b[A\n",
      "Generating train examples...: 6959 examples [00:04, 2409.94 examples/s]\u001b[A\n",
      "Generating train examples...: 7201 examples [00:04, 2346.74 examples/s]\u001b[A\n",
      "Generating train examples...: 7437 examples [00:04, 2233.02 examples/s]\u001b[A\n",
      "Generating train examples...: 7662 examples [00:04, 2182.02 examples/s]\u001b[A\n",
      "Generating train examples...: 7882 examples [00:04, 2138.57 examples/s]\u001b[A\n",
      "Generating train examples...: 8097 examples [00:04, 2129.76 examples/s]\u001b[A\n",
      "Generating train examples...: 8311 examples [00:04, 2108.48 examples/s]\u001b[A\n",
      "Generating train examples...: 8545 examples [00:05, 2175.30 examples/s]\u001b[A\n",
      "Generating train examples...: 8782 examples [00:05, 2226.44 examples/s]\u001b[A\n",
      "Generating train examples...: 9019 examples [00:05, 2268.26 examples/s]\u001b[A\n",
      "Generating train examples...: 9272 examples [00:05, 2338.78 examples/s]\u001b[A\n",
      "Generating train examples...: 9520 examples [00:05, 2374.96 examples/s]\u001b[A\n",
      "Generating train examples...: 9758 examples [00:05, 2362.43 examples/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train examples...: 10002 examples [00:05, 2385.13 examples/s]\u001b[A\n",
      "Generating train examples...: 10251 examples [00:05, 2409.46 examples/s]\u001b[A\n",
      "Generating train examples...: 10498 examples [00:05, 2427.48 examples/s]\u001b[A\n",
      "Generating train examples...: 10741 examples [00:05, 2420.71 examples/s]\u001b[A\n",
      "Generating train examples...: 10994 examples [00:06, 2446.33 examples/s]\u001b[A\n",
      "Generating train examples...: 11247 examples [00:06, 2466.27 examples/s]\u001b[A\n",
      "Generating train examples...: 11494 examples [00:06, 2460.02 examples/s]\u001b[A\n",
      "Generating train examples...: 11745 examples [00:06, 2469.29 examples/s]\u001b[A\n",
      "Generating train examples...: 11992 examples [00:06, 2454.39 examples/s]\u001b[A\n",
      "Generating train examples...: 12242 examples [00:06, 2467.89 examples/s]\u001b[A\n",
      "Generating train examples...: 12494 examples [00:06, 2483.42 examples/s]\u001b[A\n",
      "Generating train examples...: 12743 examples [00:06, 2461.41 examples/s]\u001b[A\n",
      "Generating train examples...: 12990 examples [00:06, 2427.80 examples/s]\u001b[A\n",
      "Generating train examples...: 13233 examples [00:06, 2386.53 examples/s]\u001b[A\n",
      "Generating train examples...: 13476 examples [00:07, 2398.95 examples/s]\u001b[A\n",
      "Generating train examples...: 13717 examples [00:07, 2379.30 examples/s]\u001b[A\n",
      "Generating train examples...: 13960 examples [00:07, 2387.51 examples/s]\u001b[A\n",
      "Generating train examples...: 14215 examples [00:07, 2435.15 examples/s]\u001b[A\n",
      "Generating train examples...: 14469 examples [00:07, 2458.98 examples/s]\u001b[A\n",
      "Generating train examples...: 14718 examples [00:07, 2468.19 examples/s]\u001b[A\n",
      "Generating train examples...: 14972 examples [00:07, 2489.58 examples/s]\u001b[A\n",
      "Generating train examples...: 15225 examples [00:07, 2494.26 examples/s]\u001b[A\n",
      "Generating train examples...: 15475 examples [00:07, 2481.18 examples/s]\u001b[A\n",
      "Generating train examples...: 15728 examples [00:07, 2488.31 examples/s]\u001b[A\n",
      "Generating train examples...: 15977 examples [00:08, 2435.00 examples/s]\u001b[A\n",
      "Generating train examples...: 16221 examples [00:08, 2391.97 examples/s]\u001b[A\n",
      "Generating train examples...: 16463 examples [00:08, 2399.05 examples/s]\u001b[A\n",
      "Generating train examples...: 16704 examples [00:08, 2395.08 examples/s]\u001b[A\n",
      "Generating train examples...: 16944 examples [00:08, 2375.47 examples/s]\u001b[A\n",
      "Generating train examples...: 17184 examples [00:08, 2379.00 examples/s]\u001b[A\n",
      "Generating train examples...: 17429 examples [00:08, 2399.60 examples/s]\u001b[A\n",
      "Generating train examples...: 17670 examples [00:08, 2402.68 examples/s]\u001b[A\n",
      "Generating train examples...: 17911 examples [00:08, 2355.44 examples/s]\u001b[A\n",
      "Generating train examples...: 18149 examples [00:08, 2356.02 examples/s]\u001b[A\n",
      "Generating train examples...: 18393 examples [00:09, 2380.54 examples/s]\u001b[A\n",
      "Generating train examples...: 18632 examples [00:09, 2323.19 examples/s]\u001b[A\n",
      "Generating train examples...: 18867 examples [00:09, 2325.48 examples/s]\u001b[A\n",
      "Generating train examples...: 19111 examples [00:09, 2352.54 examples/s]\u001b[A\n",
      "Generating train examples...: 19347 examples [00:09, 2280.25 examples/s]\u001b[A\n",
      "Generating train examples...: 19580 examples [00:09, 2294.39 examples/s]\u001b[A\n",
      "Generating train examples...: 19811 examples [00:09, 2298.96 examples/s]\u001b[A\n",
      "Generating train examples...: 20042 examples [00:09, 2262.12 examples/s]\u001b[A\n",
      "Generating train examples...: 20275 examples [00:09, 2275.34 examples/s]\u001b[A\n",
      "Generating train examples...: 20512 examples [00:09, 2300.75 examples/s]\u001b[A\n",
      "Generating train examples...: 20743 examples [00:10, 2296.70 examples/s]\u001b[A\n",
      "Generating train examples...: 20975 examples [00:10, 2296.80 examples/s]\u001b[A\n",
      "Generating train examples...: 21217 examples [00:10, 2331.69 examples/s]\u001b[A\n",
      "Generating train examples...: 21463 examples [00:10, 2362.89 examples/s]\u001b[A\n",
      "Generating train examples...: 21713 examples [00:10, 2396.58 examples/s]\u001b[A\n",
      "Generating train examples...: 21959 examples [00:10, 2415.46 examples/s]\u001b[A\n",
      "Generating train examples...: 22207 examples [00:10, 2431.97 examples/s]\u001b[A\n",
      "Generating train examples...: 22458 examples [00:10, 2448.09 examples/s]\u001b[A\n",
      "Generating train examples...: 22704 examples [00:10, 2444.64 examples/s]\u001b[A\n",
      "Generating train examples...: 22951 examples [00:11, 2444.92 examples/s]\u001b[A\n",
      "Generating train examples...: 23199 examples [00:11, 2455.06 examples/s]\u001b[A\n",
      "Generating train examples...: 23445 examples [00:11, 2385.29 examples/s]\u001b[A\n",
      "Generating train examples...: 23684 examples [00:11, 2379.69 examples/s]\u001b[A\n",
      "Generating train examples...: 23931 examples [00:11, 2404.67 examples/s]\u001b[A\n",
      "Generating train examples...: 24175 examples [00:11, 2415.06 examples/s]\u001b[A\n",
      "Generating train examples...: 24422 examples [00:11, 2424.18 examples/s]\u001b[A\n",
      "Generating train examples...: 24673 examples [00:11, 2449.70 examples/s]\u001b[A\n",
      "Generating train examples...: 24924 examples [00:11, 2460.33 examples/s]\u001b[A\n",
      "Generating train examples...: 25174 examples [00:11, 2464.82 examples/s]\u001b[A\n",
      "Generating train examples...: 25421 examples [00:12, 2459.01 examples/s]\u001b[A\n",
      "Generating train examples...: 25667 examples [00:12, 2441.93 examples/s]\u001b[A\n",
      "Generating train examples...: 25912 examples [00:12, 2437.06 examples/s]\u001b[A\n",
      "Generating train examples...: 26158 examples [00:12, 2437.73 examples/s]\u001b[A\n",
      "Generating train examples...: 26406 examples [00:12, 2450.14 examples/s]\u001b[A\n",
      "Generating train examples...: 26656 examples [00:12, 2457.55 examples/s]\u001b[A\n",
      "Generating train examples...: 26902 examples [00:12, 2458.27 examples/s]\u001b[A\n",
      "Generating train examples...: 27148 examples [00:12, 2410.59 examples/s]\u001b[A\n",
      "Generating train examples...: 27390 examples [00:12, 2357.73 examples/s]\u001b[A\n",
      "Generating train examples...: 27627 examples [00:12, 2327.32 examples/s]\u001b[A\n",
      "Generating train examples...: 27860 examples [00:13, 2304.19 examples/s]\u001b[A\n",
      "Generating train examples...: 28091 examples [00:13, 2294.42 examples/s]\u001b[A\n",
      "Generating train examples...: 28321 examples [00:13, 2262.60 examples/s]\u001b[A\n",
      "Generating train examples...: 28548 examples [00:13, 2258.18 examples/s]\u001b[A\n",
      "Generating train examples...: 28792 examples [00:13, 2304.82 examples/s]\u001b[A\n",
      "Generating train examples...: 29040 examples [00:13, 2355.34 examples/s]\u001b[A\n",
      "Generating train examples...: 29285 examples [00:13, 2379.69 examples/s]\u001b[A\n",
      "Generating train examples...: 29534 examples [00:13, 2405.39 examples/s]\u001b[A\n",
      "Generating train examples...: 29783 examples [00:13, 2430.55 examples/s]\u001b[A\n",
      "Generating train examples...: 30032 examples [00:13, 2448.26 examples/s]\u001b[A\n",
      "Generating train examples...: 30278 examples [00:14, 2445.93 examples/s]\u001b[A\n",
      "Generating train examples...: 30523 examples [00:14, 2432.24 examples/s]\u001b[A\n",
      "Generating train examples...: 30768 examples [00:14, 2434.12 examples/s]\u001b[A\n",
      "Generating train examples...: 31018 examples [00:14, 2453.75 examples/s]\u001b[A\n",
      "Generating train examples...: 31267 examples [00:14, 2457.56 examples/s]\u001b[A\n",
      "Generating train examples...: 31513 examples [00:14, 2443.38 examples/s]\u001b[A\n",
      "Generating train examples...: 31760 examples [00:14, 2444.32 examples/s]\u001b[A\n",
      "Generating train examples...: 32005 examples [00:14, 2416.94 examples/s]\u001b[A\n",
      "Generating train examples...: 32251 examples [00:14, 2422.51 examples/s]\u001b[A\n",
      "Generating train examples...: 32501 examples [00:14, 2440.20 examples/s]\u001b[A\n",
      "Generating train examples...: 32749 examples [00:15, 2445.06 examples/s]\u001b[A\n",
      "Generating train examples...: 32997 examples [00:15, 2447.79 examples/s]\u001b[A\n",
      "Generating train examples...: 33242 examples [00:15, 2431.95 examples/s]\u001b[A\n",
      "Generating train examples...: 33486 examples [00:15, 2371.07 examples/s]\u001b[A\n",
      "Generating train examples...: 33732 examples [00:15, 2396.98 examples/s]\u001b[A\n",
      "Generating train examples...: 33983 examples [00:15, 2423.14 examples/s]\u001b[A\n",
      "Generating train examples...: 34235 examples [00:15, 2444.54 examples/s]\u001b[A\n",
      "Generating train examples...: 34483 examples [00:15, 2447.79 examples/s]\u001b[A\n",
      "Generating train examples...: 34728 examples [00:15, 2427.22 examples/s]\u001b[A\n",
      "Generating train examples...: 34971 examples [00:15, 2391.18 examples/s]\u001b[A\n",
      "Generating train examples...: 35214 examples [00:16, 2395.74 examples/s]\u001b[A\n",
      "Generating train examples...: 35454 examples [00:16, 2383.30 examples/s]\u001b[A\n",
      "Generating train examples...: 35700 examples [00:16, 2402.81 examples/s]\u001b[A\n",
      "Generating train examples...: 35948 examples [00:16, 2425.40 examples/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train examples...: 36199 examples [00:16, 2450.50 examples/s]\u001b[A\n",
      "Generating train examples...: 36447 examples [00:16, 2459.28 examples/s]\u001b[A\n",
      "Generating train examples...: 36693 examples [00:16, 2459.45 examples/s]\u001b[A\n",
      "Generating train examples...: 36940 examples [00:16, 2455.32 examples/s]\u001b[A\n",
      "Generating train examples...: 37186 examples [00:16, 2442.50 examples/s]\u001b[A\n",
      "Generating train examples...: 37431 examples [00:17, 2430.00 examples/s]\u001b[A\n",
      "Generating train examples...: 37678 examples [00:17, 2435.77 examples/s]\u001b[A\n",
      "Generating train examples...: 37922 examples [00:17, 2422.61 examples/s]\u001b[A\n",
      "Generating train examples...: 38166 examples [00:17, 2420.52 examples/s]\u001b[A\n",
      "Generating train examples...: 38412 examples [00:17, 2425.02 examples/s]\u001b[A\n",
      "Generating train examples...: 38661 examples [00:17, 2437.49 examples/s]\u001b[A\n",
      "Generating train examples...: 38905 examples [00:17, 2437.92 examples/s]\u001b[A\n",
      "Generating train examples...: 39149 examples [00:17, 2433.63 examples/s]\u001b[A\n",
      "Generating train examples...: 39393 examples [00:17, 2413.93 examples/s]\u001b[A\n",
      "Generating train examples...: 39635 examples [00:17, 2408.27 examples/s]\u001b[A\n",
      "Generating train examples...: 39876 examples [00:18, 2389.09 examples/s]\u001b[A\n",
      "Generating train examples...: 40117 examples [00:18, 2388.80 examples/s]\u001b[A\n",
      "Generating train examples...: 40356 examples [00:18, 2221.02 examples/s]\u001b[A\n",
      "Generating train examples...: 40581 examples [00:18, 2176.26 examples/s]\u001b[A\n",
      "Generating train examples...: 40801 examples [00:18, 2111.33 examples/s]\u001b[A\n",
      "Generating train examples...: 41014 examples [00:18, 2116.57 examples/s]\u001b[A\n",
      "Generating train examples...: 41244 examples [00:18, 2162.92 examples/s]\u001b[A\n",
      "Generating train examples...: 41476 examples [00:18, 2208.44 examples/s]\u001b[A\n",
      "Generating train examples...: 41698 examples [00:18, 2205.08 examples/s]\u001b[A\n",
      "Generating train examples...: 41939 examples [00:18, 2258.74 examples/s]\u001b[A\n",
      "Generating train examples...: 42166 examples [00:19, 2245.38 examples/s]\u001b[A\n",
      "Generating train examples...: 42396 examples [00:19, 2255.96 examples/s]\u001b[A\n",
      "Generating train examples...: 42622 examples [00:19, 2214.69 examples/s]\u001b[A\n",
      "Generating train examples...: 42844 examples [00:19, 2207.35 examples/s]\u001b[A\n",
      "Generating train examples...: 43077 examples [00:19, 2240.11 examples/s]\u001b[A\n",
      "Generating train examples...: 43310 examples [00:19, 2266.41 examples/s]\u001b[A\n",
      "Generating train examples...: 43541 examples [00:19, 2279.35 examples/s]\u001b[A\n",
      "Generating train examples...: 43781 examples [00:19, 2315.22 examples/s]\u001b[A\n",
      "Generating train examples...: 44027 examples [00:19, 2351.48 examples/s]\u001b[A\n",
      "Generating train examples...: 44265 examples [00:19, 2357.05 examples/s]\u001b[A\n",
      "Generating train examples...: 44501 examples [00:20, 2316.22 examples/s]\u001b[A\n",
      "Generating train examples...: 44733 examples [00:20, 2310.74 examples/s]\u001b[A\n",
      "Generating train examples...: 44965 examples [00:20, 2293.53 examples/s]\u001b[A\n",
      "Generating train examples...: 45201 examples [00:20, 2308.33 examples/s]\u001b[A\n",
      "Generating train examples...: 45443 examples [00:20, 2336.59 examples/s]\u001b[A\n",
      "Generating train examples...: 45677 examples [00:20, 2290.33 examples/s]\u001b[A\n",
      "Generating train examples...: 45918 examples [00:20, 2325.18 examples/s]\u001b[A\n",
      "Generating train examples...: 46161 examples [00:20, 2356.10 examples/s]\u001b[A\n",
      "Generating train examples...: 46408 examples [00:20, 2382.83 examples/s]\u001b[A\n",
      "Generating train examples...: 46648 examples [00:21, 2381.21 examples/s]\u001b[A\n",
      "Generating train examples...: 46894 examples [00:21, 2397.19 examples/s]\u001b[A\n",
      "Generating train examples...: 47134 examples [00:21, 2356.04 examples/s]\u001b[A\n",
      "Generating train examples...: 47374 examples [00:21, 2368.67 examples/s]\u001b[A\n",
      "Generating train examples...: 47612 examples [00:21, 2365.00 examples/s]\u001b[A\n",
      "Generating train examples...: 47858 examples [00:21, 2386.15 examples/s]\u001b[A\n",
      "Generating train examples...: 48100 examples [00:21, 2389.41 examples/s]\u001b[A\n",
      "Generating train examples...: 48339 examples [00:21, 2382.11 examples/s]\u001b[A\n",
      "Generating train examples...: 48578 examples [00:21, 2370.03 examples/s]\u001b[A\n",
      "Generating train examples...: 48819 examples [00:21, 2381.90 examples/s]\u001b[A\n",
      "Generating train examples...: 49058 examples [00:22, 2377.20 examples/s]\u001b[A\n",
      "Generating train examples...: 49296 examples [00:22, 2362.75 examples/s]\u001b[A\n",
      "Generating train examples...: 49533 examples [00:22, 2364.74 examples/s]\u001b[A\n",
      "Generating train examples...: 49770 examples [00:22, 2365.36 examples/s]\u001b[A\n",
      "Generating train examples...: 50009 examples [00:22, 2365.35 examples/s]\u001b[A\n",
      "Generating train examples...: 50253 examples [00:22, 2380.83 examples/s]\u001b[A\n",
      "Generating train examples...: 50492 examples [00:22, 2383.56 examples/s]\u001b[A\n",
      "Generating train examples...: 50731 examples [00:22, 2343.47 examples/s]\u001b[A\n",
      "Generating train examples...: 50966 examples [00:22, 2290.73 examples/s]\u001b[A\n",
      "Generating train examples...: 51196 examples [00:22, 2286.75 examples/s]\u001b[A\n",
      "Generating train examples...: 51428 examples [00:23, 2291.27 examples/s]\u001b[A\n",
      "Generating train examples...: 51658 examples [00:23, 2242.74 examples/s]\u001b[A\n",
      "Generating train examples...: 51883 examples [00:23, 2238.05 examples/s]\u001b[A\n",
      "Generating train examples...: 52119 examples [00:23, 2267.22 examples/s]\u001b[A\n",
      "Generating train examples...: 52346 examples [00:23, 2261.32 examples/s]\u001b[A\n",
      "Generating train examples...: 52579 examples [00:23, 2281.56 examples/s]\u001b[A\n",
      "Generating train examples...: 52813 examples [00:23, 2292.17 examples/s]\u001b[A\n",
      "Generating train examples...: 53048 examples [00:23, 2302.53 examples/s]\u001b[A\n",
      "Generating train examples...: 53279 examples [00:23, 2291.36 examples/s]\u001b[A\n",
      "Generating train examples...: 53510 examples [00:23, 2290.14 examples/s]\u001b[A\n",
      "Generating train examples...: 53740 examples [00:24, 2186.07 examples/s]\u001b[A\n",
      "Generating train examples...: 53960 examples [00:24, 2136.52 examples/s]\u001b[A\n",
      "Generating train examples...: 54175 examples [00:24, 2031.80 examples/s]\u001b[A\n",
      "Generating train examples...: 54380 examples [00:24, 2019.01 examples/s]\u001b[A\n",
      "Generating train examples...: 54586 examples [00:24, 2024.88 examples/s]\u001b[A\n",
      "Generating train examples...: 54792 examples [00:24, 2034.98 examples/s]\u001b[A\n",
      "Generating train examples...: 55003 examples [00:24, 2050.88 examples/s]\u001b[A\n",
      "Generating train examples...: 55230 examples [00:24, 2109.83 examples/s]\u001b[A\n",
      "Generating train examples...: 55458 examples [00:24, 2153.67 examples/s]\u001b[A\n",
      "Generating train examples...: 55681 examples [00:25, 2175.00 examples/s]\u001b[A\n",
      "Generating train examples...: 55899 examples [00:25, 2157.26 examples/s]\u001b[A\n",
      "Generating train examples...: 56124 examples [00:25, 2180.86 examples/s]\u001b[A\n",
      "Generating train examples...: 56345 examples [00:25, 2189.51 examples/s]\u001b[A\n",
      "Generating train examples...: 56565 examples [00:25, 2113.45 examples/s]\u001b[A\n",
      "Generating train examples...: 56777 examples [00:25, 2078.26 examples/s]\u001b[A\n",
      "Generating train examples...: 56993 examples [00:25, 2096.13 examples/s]\u001b[A\n",
      "Generating train examples...: 57203 examples [00:25, 2078.72 examples/s]\u001b[A\n",
      "Generating train examples...: 57414 examples [00:25, 2082.10 examples/s]\u001b[A\n",
      "Generating train examples...: 57637 examples [00:25, 2125.35 examples/s]\u001b[A\n",
      "Generating train examples...: 57861 examples [00:26, 2157.06 examples/s]\u001b[A\n",
      "Generating train examples...: 58085 examples [00:26, 2179.32 examples/s]\u001b[A\n",
      "Generating train examples...: 58304 examples [00:26, 2144.43 examples/s]\u001b[A\n",
      "Generating train examples...: 58519 examples [00:26, 2097.52 examples/s]\u001b[A\n",
      "Generating train examples...: 58746 examples [00:26, 2147.76 examples/s]\u001b[A\n",
      "Generating train examples...: 58987 examples [00:26, 2218.39 examples/s]\u001b[A\n",
      "Generating train examples...: 59226 examples [00:26, 2262.31 examples/s]\u001b[A\n",
      "Generating train examples...: 59465 examples [00:26, 2293.34 examples/s]\u001b[A\n",
      "Generating train examples...: 59695 examples [00:26, 2142.95 examples/s]\u001b[A\n",
      "Generating train examples...: 59912 examples [00:26, 2068.51 examples/s]\u001b[A\n",
      "                                                                        \u001b[A\n",
      "Shuffling C:\\Users\\s0001033\\tensorflow_datasets\\mnist\\3.0.1.incomplete391LPE\\mnist-train.tfrecord*...:   0%|          | 0/60000 [00:00<?, ? examples/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffling C:\\Users\\s0001033\\tensorflow_datasets\\mnist\\3.0.1.incomplete391LPE\\mnist-train.tfrecord*...:   9%|▉         | 5263/60000 [00:00<00:01, 52360.53 examples/s]\u001b[A\n",
      "Shuffling C:\\Users\\s0001033\\tensorflow_datasets\\mnist\\3.0.1.incomplete391LPE\\mnist-train.tfrecord*...:  23%|██▎       | 14056/60000 [00:00<00:00, 72812.58 examples/s]\u001b[A\n",
      "Shuffling C:\\Users\\s0001033\\tensorflow_datasets\\mnist\\3.0.1.incomplete391LPE\\mnist-train.tfrecord*...:  38%|███▊      | 22731/60000 [00:00<00:00, 79133.51 examples/s]\u001b[A\n",
      "Shuffling C:\\Users\\s0001033\\tensorflow_datasets\\mnist\\3.0.1.incomplete391LPE\\mnist-train.tfrecord*...:  52%|█████▏    | 31180/60000 [00:00<00:00, 81118.23 examples/s]\u001b[A\n",
      "Shuffling C:\\Users\\s0001033\\tensorflow_datasets\\mnist\\3.0.1.incomplete391LPE\\mnist-train.tfrecord*...:  65%|██████▌   | 39293/60000 [00:00<00:00, 80832.31 examples/s]\u001b[A\n",
      "Shuffling C:\\Users\\s0001033\\tensorflow_datasets\\mnist\\3.0.1.incomplete391LPE\\mnist-train.tfrecord*...:  80%|███████▉  | 47768/60000 [00:00<00:00, 81878.69 examples/s]\u001b[A\n",
      "Shuffling C:\\Users\\s0001033\\tensorflow_datasets\\mnist\\3.0.1.incomplete391LPE\\mnist-train.tfrecord*...:  93%|█████████▎| 55956/60000 [00:00<00:00, 79562.73 examples/s]\u001b[A\n",
      "Generating splits...:  50%|█████     | 1/2 [00:27<00:27, 27.86s/ splits]                                                                                              \u001b[A\n",
      "Generating test examples...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "Generating test examples...: 164 examples [00:00, 1639.54 examples/s]\u001b[A\n",
      "Generating test examples...: 391 examples [00:00, 2008.73 examples/s]\u001b[A\n",
      "Generating test examples...: 613 examples [00:00, 2101.87 examples/s]\u001b[A\n",
      "Generating test examples...: 853 examples [00:00, 2218.51 examples/s]\u001b[A\n",
      "Generating test examples...: 1090 examples [00:00, 2271.64 examples/s]\u001b[A\n",
      "Generating test examples...: 1329 examples [00:00, 2303.87 examples/s]\u001b[A\n",
      "Generating test examples...: 1565 examples [00:00, 2321.26 examples/s]\u001b[A\n",
      "Generating test examples...: 1800 examples [00:00, 2330.06 examples/s]\u001b[A\n",
      "Generating test examples...: 2034 examples [00:00, 2326.20 examples/s]\u001b[A\n",
      "Generating test examples...: 2267 examples [00:01, 2312.80 examples/s]\u001b[A\n",
      "Generating test examples...: 2502 examples [00:01, 2317.42 examples/s]\u001b[A\n",
      "Generating test examples...: 2734 examples [00:01, 2283.48 examples/s]\u001b[A\n",
      "Generating test examples...: 2963 examples [00:01, 2243.89 examples/s]\u001b[A\n",
      "Generating test examples...: 3188 examples [00:01, 2218.12 examples/s]\u001b[A\n",
      "Generating test examples...: 3422 examples [00:01, 2247.68 examples/s]\u001b[A\n",
      "Generating test examples...: 3658 examples [00:01, 2276.55 examples/s]\u001b[A\n",
      "Generating test examples...: 3886 examples [00:01, 2270.57 examples/s]\u001b[A\n",
      "Generating test examples...: 4114 examples [00:01, 2246.99 examples/s]\u001b[A\n",
      "Generating test examples...: 4339 examples [00:01, 2213.23 examples/s]\u001b[A\n",
      "Generating test examples...: 4561 examples [00:02, 2208.73 examples/s]\u001b[A\n",
      "Generating test examples...: 4782 examples [00:02, 2164.19 examples/s]\u001b[A\n",
      "Generating test examples...: 5030 examples [00:02, 2249.86 examples/s]\u001b[A\n",
      "Generating test examples...: 5259 examples [00:02, 2258.77 examples/s]\u001b[A\n",
      "Generating test examples...: 5493 examples [00:02, 2282.49 examples/s]\u001b[A\n",
      "Generating test examples...: 5727 examples [00:02, 2293.94 examples/s]\u001b[A\n",
      "Generating test examples...: 5957 examples [00:02, 2269.12 examples/s]\u001b[A\n",
      "Generating test examples...: 6185 examples [00:02, 2244.21 examples/s]\u001b[A\n",
      "Generating test examples...: 6410 examples [00:02, 2232.77 examples/s]\u001b[A\n",
      "Generating test examples...: 6634 examples [00:02, 2228.04 examples/s]\u001b[A\n",
      "Generating test examples...: 6857 examples [00:03, 2183.38 examples/s]\u001b[A\n",
      "Generating test examples...: 7076 examples [00:03, 2172.91 examples/s]\u001b[A\n",
      "Generating test examples...: 7294 examples [00:03, 2156.91 examples/s]\u001b[A\n",
      "Generating test examples...: 7512 examples [00:03, 2157.30 examples/s]\u001b[A\n",
      "Generating test examples...: 7728 examples [00:03, 2120.69 examples/s]\u001b[A\n",
      "Generating test examples...: 7941 examples [00:03, 2068.86 examples/s]\u001b[A\n",
      "Generating test examples...: 8149 examples [00:03, 2030.93 examples/s]\u001b[A\n",
      "Generating test examples...: 8380 examples [00:03, 2105.38 examples/s]\u001b[A\n",
      "Generating test examples...: 8597 examples [00:03, 2123.88 examples/s]\u001b[A\n",
      "Generating test examples...: 8827 examples [00:03, 2169.30 examples/s]\u001b[A\n",
      "Generating test examples...: 9071 examples [00:04, 2242.54 examples/s]\u001b[A\n",
      "Generating test examples...: 9314 examples [00:04, 2291.23 examples/s]\u001b[A\n",
      "Generating test examples...: 9544 examples [00:04, 2286.21 examples/s]\u001b[A\n",
      "Generating test examples...: 9773 examples [00:04, 2264.82 examples/s]\u001b[A\n",
      "                                                                      \u001b[A\n",
      "Shuffling C:\\Users\\s0001033\\tensorflow_datasets\\mnist\\3.0.1.incomplete391LPE\\mnist-test.tfrecord*...:   0%|          | 0/10000 [00:00<?, ? examples/s]\u001b[A\n",
      "Shuffling C:\\Users\\s0001033\\tensorflow_datasets\\mnist\\3.0.1.incomplete391LPE\\mnist-test.tfrecord*...:  86%|████████▌ | 8614/10000 [00:00<00:00, 85288.68 examples/s]\u001b[A\n",
      "                                                                                                                                                                    \u001b[A\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to C:\\Users\\s0001033\\tensorflow_datasets\\mnist\\3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# remember the comment from above\n",
    "# these datasets will be stored in C:\\Users\\*USERNAME*\\tensorflow_datasets\\...\n",
    "# the first time you download a dataset, it is stored in the respective folder \n",
    "# every other time, it is automatically loading the copy on your computer \n",
    "\n",
    "# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) \n",
    "# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument\n",
    "# there are other arguments we can specify, which we can find useful\n",
    "# mnist_dataset = tfds.load(name='mnist', as_supervised=True)\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "# with_info=True will also provide us with a tuple containing information about the version, features, number of samples\n",
    "# we will use this information a bit below and we will store it in mnist_info\n",
    "\n",
    "# as_supervised=True will load the dataset in a 2-tuple structure (input, target) \n",
    "# alternatively, as_supervised=False, would return a dictionary\n",
    "# obviously we prefer to have our inputs and targets separated \n",
    "\n",
    "# once we have loaded the dataset, we can easily extract the training and testing dataset with the built references\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "# by default, TF has training and testing datasets, but no validation sets\n",
    "# thus we must split it on our own\n",
    "\n",
    "# we start by defining the number of validation samples as a % of the train samples\n",
    "# this is also where we make use of mnist_info (we don't have to count the observations)\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "# let's cast this number to an integer, as a float may cause an error along the way\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "# let's also store the number of test samples in a dedicated variable (instead of using the mnist_info one)\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "# once more, we'd prefer an integer (rather than the default float)\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "\n",
    "# normally, we would like to scale our data in some way to make the result more numerically stable\n",
    "# in this case we will simply prefer to have inputs between 0 and 1\n",
    "# let's define a function called: scale, that will take an MNIST image and its label\n",
    "def scale(image, label):\n",
    "    # we make sure the value is a float\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)\n",
    "    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1 \n",
    "    image /= 255.\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "# the method .map() allows us to apply a custom transformation to a given dataset\n",
    "# we have already decided that we will get the validation data from mnist_train, so \n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "# finally, we scale and batch the test data\n",
    "# we scale it so it has the same magnitude as the train and validation\n",
    "# there is no need to shuffle it, because we won't be training on the test data\n",
    "# there would be a single batch, equal to the size of the test data\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "\n",
    "# let's also shuffle the data\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "# this BUFFER_SIZE parameter is here for cases when we're dealing with enormous datasets\n",
    "# then we can't shuffle the whole dataset in one go because we can't fit it all in memory\n",
    "# so instead TF only stores BUFFER_SIZE samples in memory at a time and shuffles them\n",
    "# if BUFFER_SIZE=1 => no shuffling will actually happen\n",
    "# if BUFFER_SIZE >= num samples => shuffling is uniform\n",
    "# BUFFER_SIZE in between - a computational optimization to approximate uniform shuffling\n",
    "\n",
    "# luckily for us, there is a shuffle method readily available and we just need to specify the buffer size\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# once we have scaled and shuffled the data, we can proceed to actually extracting the train and validation\n",
    "# our validation data would be equal to 10% of the training set, which we've already calculated\n",
    "# we use the .take() method to take that many samples\n",
    "# finally, we create a batch with a batch size equal to the total number of validation samples\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "# similarly, the train_data is everything else, so we skip as many samples as there are in the validation dataset\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "# determine the batch size\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# we can also take advantage of the occasion to batch the train data\n",
    "# this would be very helpful when we train, as we would be able to iterate over the different batches\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "\n",
    "# batch the test data\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "\n",
    "# takes next batch (it is the only batch)\n",
    "# because as_supervized=True, we've got a 2-tuple structure\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline the model\n",
    "When thinking about a deep learning algorithm, we mostly imagine building the model. So, let's do it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "# Use same hidden layer size for both hidden layers. Not a necessity.\n",
    "hidden_layer_size = 50\n",
    "    \n",
    "# define how the model will look like\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    # the first layer (the input layer)\n",
    "    # each observation is 28x28x1 pixels, therefore it is a tensor of rank 3\n",
    "    # since we don't know CNNs yet, we don't know how to feed such input into our net, so we must flatten the images\n",
    "    # there is a convenient method 'Flatten' that simply takes our 28x28x1 tensor and orders it into a (None,) \n",
    "    # or (28x28x1,) = (784,) vector\n",
    "    # this allows us to actually create a feed forward neural network\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer\n",
    "    \n",
    "    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n",
    "    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    \n",
    "    # the final layer is no different, we just make sure to activate it with softmax\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the optimizer we'd like to use, \n",
    "# the loss function, \n",
    "# and the metrics we are interested in obtaining at each iteration\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "That's where we train the model we have built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 2s - loss: 0.4111 - accuracy: 0.8825 - val_loss: 0.2139 - val_accuracy: 0.9362\n",
      "Epoch 2/5\n",
      "540/540 - 1s - loss: 0.1872 - accuracy: 0.9446 - val_loss: 0.1568 - val_accuracy: 0.9543\n",
      "Epoch 3/5\n",
      "540/540 - 1s - loss: 0.1422 - accuracy: 0.9584 - val_loss: 0.1215 - val_accuracy: 0.9635\n",
      "Epoch 4/5\n",
      "540/540 - 1s - loss: 0.1139 - accuracy: 0.9660 - val_loss: 0.1060 - val_accuracy: 0.9670\n",
      "Epoch 5/5\n",
      "540/540 - 1s - loss: 0.0964 - accuracy: 0.9714 - val_loss: 0.0910 - val_accuracy: 0.9702\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16099a167f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the maximum number of epochs\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# we fit the model, specifying the\n",
    "# training data\n",
    "# the total number of epochs\n",
    "# and the validation data we just created ourselves in the format: (inputs,targets)\n",
    "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose =2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "As we discussed in the lectures, after training on the training data and validating on the validation data, we test the final prediction power of our model by running it on the test dataset that the algorithm has NEVER seen before.\n",
    "\n",
    "It is very important to realize that fiddling with the hyperparameters overfits the validation dataset. \n",
    "\n",
    "The test is the absolute final instance. You should not test before you are completely done with adjusting your model.\n",
    "\n",
    "If you adjust your model after testing, you will start overfitting the test dataset, which will defeat its purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 183ms/step - loss: 0.1015 - accuracy: 0.9697\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.10. Test accuracy: 96.97%\n"
     ]
    }
   ],
   "source": [
    "# We can apply some nice formatting if we want to\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the initial model and hyperparameters given in this notebook, the final test accuracy should be roughly around 97%.\n",
    "\n",
    "Each time the code is rerun, we get a different accuracy as the batches are shuffled, the weights are initialized in a different way, etc.\n",
    "\n",
    "Finally, we have intentionally reached a suboptimal solution, so you can have space to build on it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0_ver3]",
   "language": "python",
   "name": "conda-env-py3-TF2.0_ver3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
